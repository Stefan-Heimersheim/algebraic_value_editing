# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     custom_cell_magics: kql
#     formats: ipynb,py
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: remix
#     language: python
#     name: python3
# ---

# # Exponential growth of residual stream norms

try:
    import algebraic_value_editing
except ImportError:
    commit = "15bcf55"  # Stable commit
    get_ipython().run_line_magic(  # type: ignore
        magic_name="pip",
        line=(
            "install -U"
            f" git+https://github.com/montemac/algebraic_value_editing.git@{commit}"
        ),
    )


# +
import torch
import pandas as pd
from typing import List, Dict
import matplotlib.pyplot as plt
import os
from fancy_einsum import einsum

from transformer_lens.HookedTransformer import HookedTransformer

from algebraic_value_editing import hook_utils, prompt_utils
from algebraic_value_editing.prompt_utils import RichPrompt


# +
device: str = "cpu"
model_name = "gpt2-xl"
model: HookedTransformer = HookedTransformer.from_pretrained(model_name, device="cpu")
_ = model.to(device)

_ = torch.set_grad_enabled(False)
torch.manual_seed(0)  # For reproducibility
# -


# Let's examine what the residual stream magnitudes tend to be, by taking the Frobenius
# norm of the residual stream at each sequence position. We'll do this for
# a range of prompts at a range of locations in the forward pass. (The
# downloaded prompts were mostly generated by GPT-4.)

# +
import requests

url = "https://raw.githubusercontent.com/montemac/algebraic_value_editing/cb6b1a42493a385ca02e7b9e6bbcb9bff9d006dc/scripts/prompts.txt"  # Cached at a commit to prevent changing results

response = requests.get(url)

if response.status_code == 200:
    # If the request is successful, split the content by line breaks to create a list of strings
    prompts = response.text.splitlines()
    print(f"Downloaded {len(prompts)} prompts")
else:
    raise Exception(
        f"Failed to download the file: {response.status_code} -" f" {response.reason}"
    )


# +
DF_COLS: List[str] = [
    "Prompt",
    "Activation Location",
    "Activation Name",
    "Magnitude",
]

sampling_kwargs: Dict[str, float] = {
    "temperature": 1.0,
    "top_p": 0.3,
    "freq_penalty": 1.0,
}

num_layers: int = model.cfg.n_layers
# -


# ## Residual stream magnitudes increase exponentially with layer number
# As the forward pass progresses through the network, the residual
# stream tends to increase in magnitude in an exponential fashion. This
# is easily visible in the histogram below, which shows the distribution
# of residual stream magnitudes for each layer of the network. The activation
# distribution translates by an almost constant factor each 6 layers,
# and the x-axis (magnitude) is log-scale, so magnitude apparently
# increases exponentially with layer number.*
#
# (Intriguingly, there are a few outlier residual streams which have
# magnitude over an order of magnitude larger than the rest.)
#
# Alex's first guess for the exponential magnitude increase was: Each OV circuit is a linear function of the
# residual stream given a fixed attention pattern. Then you add the head
# OV outputs back into a residual stream, which naively doubles the
# magnitude assuming the OV outputs have similar norm to the input
# residual stream. The huge problem with this explanation is layernorm,
# which is applied to the inputs to the attention and MLP layers. This
# should basically whiten the input to the OV circuits if the gain
# parameters are close to 1.
#
# \* Stefan Heimersheim previously noticed this phenomenon in GPT2-small.

# +
import plotly.express as px
import plotly.graph_objects as go
import numpy as np


def magnitude_histogram(df: pd.DataFrame, title="Residual Stream Magnitude by Layer Number",
    xaxis_title="log10 Residual Stream norm", yaxis_title="Percentage of residual streams") -> go.Figure:
    """Plot a histogram of the residual stream magnitudes for each layer
    of the network."""
    assert (
        "Magnitude" in df.columns
    ), "Dataframe must have a 'Magnitude' column"

    df["LogMagnitude"] = np.log10(df["Magnitude"])

    # Get the number of unique activation locations
    num_unique_activation_locations = df["Activation Location"].nunique()

    # Generate a color list that is long enough to accommodate all unique activation locations
    extended_rainbow = (
        px.colors.sequential.Rainbow * num_unique_activation_locations
    )
    color_list = extended_rainbow[:num_unique_activation_locations][::-1]

    fig = px.histogram(
        df,
        x="LogMagnitude",
        color="Activation Location",
        marginal="rug",
        histnorm="percent",
        nbins=100,
        opacity=0.5,
        barmode="overlay",
        color_discrete_sequence=color_list,
    )

    fig.update_layout(
        legend_title_text="After Layer Number",
        title=title,
        xaxis_title=xaxis_title,
        yaxis_title=yaxis_title,
    )

    return fig


# -

rerun_slow_plots = False

if rerun_slow_plots:
    # Create an empty dataframe with the required columns
    prompt_df = pd.DataFrame(columns=DF_COLS)
    
    from algebraic_value_editing import prompt_utils
    
    # Loop through activation locations and prompts
    activation_locations_8: List[int] = [5, 10, 15, 20, 25, 30, 35, 40, 45]
    
    for act_loc in activation_locations_8:
        act_name: str = prompt_utils.get_block_name(block_num=act_loc)
        for prompt in prompts:
            mags: torch.Tensor = hook_utils.prompt_magnitudes(
                model=model, prompt=prompt, act_name=act_name
            ).cpu()
    
            # Create a new dataframe row with the current data
            row = pd.DataFrame(
                {
                    "Prompt": prompt,
                    "Activation Location": act_loc,
                    "Activation Name": act_name,
                    "Magnitude": mags[1:], #remove BOS
                }
            )
    
            # Append the new row to the dataframe
            prompt_df = pd.concat([prompt_df, row], ignore_index=True)

if rerun_slow_plots:
    fig: go.Figure = magnitude_histogram(prompt_df)
    fig.show()

# In GPT2-XL, the fast magnitude gain
# occurs in the first 7 layers. Let's find out where.

if rerun_slow_plots:
    activation_locations: List[int] = list(range(7))
    first_7_df = pd.DataFrame(columns=DF_COLS)
    
    for act_loc in activation_locations:
        prefixes = ["pre", "mid", "post"] if act_loc == 0 else ["mid", "post"]
        for prefix in prefixes:
            act_name = f"blocks.{act_loc}.hook_resid_{prefix}"
            for prompt in prompts:
                mags: torch.Tensor = hook_utils.prompt_magnitudes(
                    model=model, prompt=prompt, act_name=act_name
                ).cpu()
                loc_delta = 0 if prefix == "pre" else 0.5 if prefix == "mid" else 1
                # Create a new dataframe row with the current data
                row = pd.DataFrame(
                    {
                        "Prompt": prompt,
                        "Activation Location": act_loc + loc_delta,
                        "Activation Name": act_name,
                        "Magnitude": mags[1:],
                    }
                )
    
                # Append the new row to the dataframe
                first_7_df = pd.concat([first_7_df, row], ignore_index=True)

if rerun_slow_plots:
    fig: go.Figure = magnitude_histogram(first_7_df)
    fig.show()


# Most of the jump happens after the 0th layer in the transformer, and
# a smaller jump happens between the 1st and 2nd layers.

# ## Plotting residual stream magnitudes against layer number
# Let's zoom in on how specific token magnitudes evolve over a forward
# pass. It turns out that the zeroth position (the `<|endoftext|>` token) has a much larger
# magnitude than the rest. (This possibly explains the outlier
# magnitudes for the prompt histograms.)

# +
def line_plot(
    df: pd.DataFrame,
    log_y: bool = True,
    title: str = "Residual Stream Norm by Layer Number",
    color: str = "Prompt",
    legend_title_text: str = "Prompt",
) -> go.Figure:
    """Make a line plot of the RichPrompt norm. If log_y is True,
    adds a column to the dataframe with the log10 of the norm."""
    for col in ["Prompt", "Activation Location", "Magnitude"]:
        assert col in df.columns, f"Column {col} not in dataframe"

    if log_y:
        df["LogMagnitude"] = np.log10(df["Magnitude"])

    fig = px.line(
        df,
        x="Activation Location",
        y="LogMagnitude" if log_y else "Magnitude",
        color=color,
        color_discrete_sequence=px.colors.sequential.Rainbow[::-1],
    )

    fig.update_layout(
        legend_title_text=legend_title_text,
        title=title,
        xaxis_title="Layer Number",
        yaxis_title=f"Norm{' (log 10)' if log_y else ''}",
    )

    return fig


# -

if rerun_slow_plots:
    # Create an empty dataframe with the required columns
    all_resid_pre_locations: List[int] = torch.arange(0, num_layers, 1).tolist()
    addition_df = pd.DataFrame(columns=DF_COLS)

    # Loop through activation locations and prompts
    for act_loc in all_resid_pre_locations:
        act_name: str = prompt_utils.get_block_name(block_num=act_loc)

        for context in ("MATS is really cool",):
            mags: torch.Tensor = hook_utils.prompt_magnitudes(
                model=model, prompt=context, act_name=act_name
            ).cpu()
            str_tokens: List[str] = model.to_str_tokens(context)

            for pos, mag in enumerate(mags):
                # Create a new dataframe row with the current data
                row = pd.DataFrame(
                    {
                        "Token": [str_tokens[pos]],
                        "Activation Location": [act_loc],
                        "Activation Name": [act_name],
                        "Magnitude": [mag],
                    }
                )

                # Append the new row to the dataframe
                addition_df = pd.concat([addition_df, row], ignore_index=True)

if rerun_slow_plots:
    for use_log in (True, False):
        fig = line_plot(
            addition_df,
            log_y=use_log,
            title=f"Residual Stream Norm by Layer Number in {model_name}",
            color="Token",
        )
        fig.update_layout(width=600)
        fig.show()

# To confirm the exponential increase in magnitude, let's plot the
# Frobenius
# norm of the residual stream at position `i` just before layer `t`,
# divided by the norm before `t-1`.

if rerun_slow_plots:
    # Make a plotly line plot of the relative magnitudes vs layer
    # number, with color representing the token location of the "MATS is
    # really cool" prompt

    # Create an empty dataframe with the required columns
    all_resid_pre_locations: List[int] = torch.arange(1, num_layers, 1).tolist()
    relative_df = pd.DataFrame(columns=DF_COLS)
    MATS_prompt: str = "MATS is really cool"

    mags_prev: torch.Tensor = hook_utils.prompt_magnitudes(
        model=model, prompt=MATS_prompt, act_name=prompt_utils.get_block_name(0)
    ).cpu()

    # Loop through activation locations and prompts
    for act_loc in all_resid_pre_locations:
        act_name: str = prompt_utils.get_block_name(block_num=act_loc)
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=MATS_prompt, act_name=act_name
        ).cpu()

        tokens: List[str] = model.to_str_tokens(MATS_prompt)
        for pos, mag in enumerate(mags):
            # Create a new dataframe row with the current data
            row = pd.DataFrame(
                {
                    "Prompt": [tokens[pos]],
                    "Activation Location": [act_loc],
                    "Activation Name": [act_name],
                    "Magnitude": [mag / mags_prev[pos]],
                }
            )

            # Append the new row to the dataframe
            relative_df = pd.concat([relative_df, row], ignore_index=True)

        mags_prev = mags

if rerun_slow_plots:
    relative_fig = line_plot(
        relative_df,
        log_y=False,
        title=f"Norm(n)/Norm(n-1) across layers n in {model_name}",
        legend_title_text="Token",
    )

    # Set y label to be "Norm growth rate"
    relative_fig.update_yaxes(title_text="Norm growth rate")

    # Set y bounds to [.9, 1.5]
    relative_fig.update_yaxes(range=[0.9, 1.5])

    # Plot a horizontal line at y=1
    relative_fig.add_hline(y=1, line_dash="dash", line_color="black")
    relative_fig.update_layout(width=600)

    relative_fig.show()

# ## Stefan's original findings

small_model_name = "gpt2-small"
small_model = HookedTransformer.from_pretrained(small_model_name)
small_cache = small_model.run_with_cache(prompts)[1]

# +
# Extract (A) all standard deviations, and (B) mean standard
# deviations across non-BOS non-padding positions.

stds = [] #A
meanstds = [] #B
for layer in range(small_model.cfg.n_layers):
    # A
    std = small_cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1)
    stds.append(std)
    # B
    stds_non_BOS_non_PAD = []
    for batch in range(len(prompts)):
        prompt_len = len(small_model.to_str_tokens(prompts[batch]))
        for pos in range(1, prompt_len): # Go from 1 (avoid BOS) to prompt_len (avoid PAD)
            stds_non_BOS_non_PAD.append(std[batch, pos].item())
    meanstds.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())

plt.figure(figsize=(8,4))
plt.plot(range(small_model.cfg.n_layers), meanstds, marker="o", label=small_model_name)
plt.title("Mean Standard Deviation of Residual Stream activations after layer $N$")
plt.xlabel("Layer Index $N$")
plt.ylabel("Mean Standard Deviation")
plt.legend()
plt.savefig(f"stds_{small_model_name}.png")
plt.show()
# -

# ## Analysis section of the post
#
# Covering **Comparison of various transformer models**, **Theories for the source of the growth**, and **Analyzing the model weights to understand the behaviour**

# ### Compute and store values

assert model_name == "gpt2-xl"
assert model.cfg.model_name == model_name
cache = model.run_with_cache(prompts)[1]

# For BOS and padding tokens

# +
# Collect stds by position
cache = model.run_with_cache(prompts)[1]
stds_by_pos = []
norms_by_pos = []
for layer in range(model.cfg.n_layers):
    stds_by_pos.append(cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1))
    norms_by_pos.append(cache[f"blocks.{layer}.hook_resid_post"].norm(dim=-1))

stds_by_pos = np.array(stds_by_pos)
norms_by_pos = np.array(norms_by_pos)
# -

# For Comparison of various transformer models

# +
for test_model_name in ["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                        "opt-125m", "opt-1.3b", "opt-2.7b",\
                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                        "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]:
    print(test_model_name)
    # Check if file exists
    if os.path.isfile(f"cached_data/meannorms_mlp_out_{test_model_name}.npy"):
        continue
    test_model = HookedTransformer.from_pretrained(
        test_model_name, device="cpu"
    )
    test_cache = test_model.run_with_cache(prompts)[1]
    meanstds_pre, meanstds_mid, meanstds_post, meanstds_attn_out, meanstds_mlp_out, \
        meannorms_pre, meannorms_mid, meannorms_post, meannorms_attn_out, meannorms_mlp_out \
        = [], [], [], [], [], [], [], [], [], []
    for layer in range(test_model.cfg.n_layers):
        pre = test_cache[f"blocks.{layer}.hook_resid_pre"]
        mid = test_cache[f"blocks.{layer}.hook_resid_mid"] if not "pythia" in test_model_name else pre
        post = test_cache[f"blocks.{layer}.hook_resid_post"]
        attn_out = test_cache[f"blocks.{layer}.hook_attn_out"]
        mlp_out = test_cache[f"blocks.{layer}.hook_mlp_out"]
        stds_pre_non_BOS_non_PAD, stds_mid_non_BOS_non_PAD, stds_post_non_BOS_non_PAD, \
            stds_attn_out_non_BOS_non_PAD, stds_mlp_out_non_BOS_non_PAD, \
            norms_pre_non_BOS_non_PAD, norms_mid_non_BOS_non_PAD, norms_post_non_BOS_non_PAD, \
            norms_attn_out_non_BOS_non_PAD, norms_mlp_out_non_BOS_non_PAD \
            = [], [], [], [], [], [], [], [], [], []
        for batch in range(len(prompts)):
            prompt_len = len(test_model.to_str_tokens(prompts[batch]))
            for pos in range(1, prompt_len):
                norms_pre_non_BOS_non_PAD.append(pre[batch, pos].norm(dim=-1).item())
                norms_mid_non_BOS_non_PAD.append(mid[batch, pos].norm(dim=-1).item())
                norms_post_non_BOS_non_PAD.append(post[batch, pos].norm(dim=-1).item())
                norms_attn_out_non_BOS_non_PAD.append(attn_out[batch, pos].norm(dim=-1).item())
                norms_mlp_out_non_BOS_non_PAD.append(mlp_out[batch, pos].norm(dim=-1).item())
                stds_pre_non_BOS_non_PAD.append(pre[batch, pos].std(dim=-1).item())
                stds_mid_non_BOS_non_PAD.append(mid[batch, pos].std(dim=-1).item())
                stds_post_non_BOS_non_PAD.append(post[batch, pos].std(dim=-1).item())
                stds_attn_out_non_BOS_non_PAD.append(attn_out[batch, pos].std(dim=-1).item())
                stds_mlp_out_non_BOS_non_PAD.append(mlp_out[batch, pos].std(dim=-1).item())
        meannorms_pre.append(torch.tensor(norms_pre_non_BOS_non_PAD).mean().item())
        meannorms_mid.append(torch.tensor(norms_mid_non_BOS_non_PAD).mean().item())
        meannorms_post.append(torch.tensor(norms_post_non_BOS_non_PAD).mean().item())
        meannorms_attn_out.append(torch.tensor(norms_attn_out_non_BOS_non_PAD).mean().item())
        meannorms_mlp_out.append(torch.tensor(norms_mlp_out_non_BOS_non_PAD).mean().item())
        meanstds_pre.append(torch.tensor(stds_pre_non_BOS_non_PAD).mean().item())
        meanstds_mid.append(torch.tensor(stds_mid_non_BOS_non_PAD).mean().item())
        meanstds_post.append(torch.tensor(stds_post_non_BOS_non_PAD).mean().item())
        meanstds_attn_out.append(torch.tensor(stds_attn_out_non_BOS_non_PAD).mean().item())
        meanstds_mlp_out.append(torch.tensor(stds_mlp_out_non_BOS_non_PAD).mean().item())
    # Save to file
    if not os.path.isdir("cached_data"):
        os.mkdir("cached_data")
    np.save(f"cached_data/meannorms_pre_{test_model_name}.npy", meannorms_pre)
    np.save(f"cached_data/meannorms_mid_{test_model_name}.npy", meannorms_mid)
    np.save(f"cached_data/meannorms_post_{test_model_name}.npy", meannorms_post)
    np.save(f"cached_data/meannorms_attn_out_{test_model_name}.npy", meannorms_attn_out)
    np.save(f"cached_data/meannorms_mlp_out_{test_model_name}.npy", meannorms_mlp_out)
    np.save(f"cached_data/meanstds_pre_{test_model_name}.npy", meanstds_pre)
    np.save(f"cached_data/meanstds_mid_{test_model_name}.npy", meanstds_mid)
    np.save(f"cached_data/meanstds_post_{test_model_name}.npy", meanstds_post)
    np.save(f"cached_data/meanstds_attn_out_{test_model_name}.npy", meanstds_attn_out)
    np.save(f"cached_data/meanstds_mlp_out_{test_model_name}.npy", meanstds_mlp_out)


# -

# For Theories for the source of the growth

# +
# Collect mean std and norm from resid_pre/post, mid
meanstds = []
meanstds_mid = []
meanstds_attn_out = []
meanstds_mlp_out = []

for layer in range(model.cfg.n_layers):
    hooks = ["resid_mid", "resid_post", "attn_out", "mlp_out"]
    if layer == 0:
        hooks = ["resid_pre", *hooks]
    
    for hook in hooks:
        acts = cache[f"blocks.{layer}.hook_"+hook]
        assert torch.allclose(torch.tensor(0.), acts.mean(dim=-1), rtol=1e-2, atol=1e-4)
        std = acts.std(dim=-1)
        norm = acts.norm(dim=-1)
        assert torch.allclose(norm, std*np.sqrt(model.cfg.d_model), rtol=1e-2)
        stds_non_BOS_non_PAD = []
        for batch in range(len(prompts)):
            prompt_len = len(model.to_str_tokens(prompts[batch]))
            for pos in range(1, prompt_len):
                stds_non_BOS_non_PAD.append(std[batch, pos].item())
        if hook == "resid_mid":
            meanstds_mid.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())
        elif hook == "resid_post" or hook == "resid_pre":
            meanstds.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())
        elif hook == "attn_out":
            meanstds_attn_out.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())
        elif hook == "mlp_out":
            meanstds_mlp_out.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())
        else:
            raise ValueError(f"Unknown hook {hook}")

meanstds = np.array(meanstds)
meanstds_mid = np.array(meanstds_mid)
meanstds_attn_out = np.array(meanstds_attn_out)
meanstds_mlp_out = np.array(meanstds_mlp_out)

meannorms = np.sqrt(model.cfg.d_model)*meanstds
meannorms_mid = np.sqrt(model.cfg.d_model)*meanstds_mid
meannorms_attn_out = np.sqrt(model.cfg.d_model)*meanstds_attn_out
meannorms_mlp_out = np.sqrt(model.cfg.d_model)*meanstds_mlp_out
# -

# ### Comparison of various transformer models

# +
fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(6,8), constrained_layout=True, sharey=True)
for i, test_model_name in enumerate(["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                                     "opt-125m", "opt-1.3b", "opt-2.7b",\
                                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                                            "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]):
    meannorms_test = np.load(f"cached_data/meannorms_post_{test_model_name}.npy")
    ax = ax1 if i < 8 else ax2
    ax.semilogy(range(len(meannorms_test)), meannorms_test, marker="o", label=test_model_name, alpha=0.5)
    # Fit line (by eye)
    if test_model_name=="gpt2-xl":
        min_layer=5
        max_layer=41
        slope = (np.log10(meannorms_test[max_layer])-np.log10(meannorms_test[min_layer]))/(max_layer-min_layer)
        print(f"Factor per layer: {10**(slope):.3f}")
        ax.plot((min_layer, max_layer), [meannorms_test[i] for i in (min_layer, max_layer)], color="orange", label=f"gpt2-xl slope: {10**slope:.1%}", alpha=1)

fig.suptitle("Norm of Residual Stream activations after layer $N$")
ax1.set_title("OpenAI and Facebook models")
ax2.set_title("EleutherAI models")
ax1.set_xlabel("Layer index $N$")
ax2.set_xlabel("Layer index $N$")
ax1.set_ylabel("Residual stream vector L2-norm")
ax2.set_ylabel("Residual stream vector L2-norm")
ax2.set_xlim(ax1.get_xlim())
ax1.legend()
ax2.legend()
fig.savefig(f"norms_all.png")
plt.show()

# +
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12,5), constrained_layout=True, sharey=True)
for i, test_model_name in enumerate(["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                                     "opt-125m", "opt-1.3b", "opt-2.7b",\
                                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                                            "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]):
    meanstds_post = np.load(f"cached_data/meanstds_post_{test_model_name}.npy")
    ax = ax1 if i < 8 else ax2
    ax.semilogy(range(len(meanstds_post)), meanstds_post, marker="o", label=test_model_name, alpha=0.5)
    # Fit line (by eye)
    if test_model_name=="gpt2-xl":
        min_layer=5
        max_layer=41
        slope = (np.log10(meanstds_post[max_layer])-np.log10(meanstds_post[min_layer]))/(max_layer-min_layer)
        print(f"Factor per layer: {10**(slope):.3f}")
        # Default color
        c = ax.lines[-1].get_color()
        ax.plot((min_layer, max_layer), [meanstds_post[i] for i in (min_layer, max_layer)], color=c, label=f"gpt2-xl slope: {10**slope:.1%}", alpha=0.5)

fig.suptitle("Mean Standard Deviation of Residual Stream activations after layer $N$")
ax1.set_title("OpenAI and Facebook models")
ax2.set_title("EleutherAI models")
ax1.set_xlabel("Layer index $N$")
ax2.set_xlabel("Layer index $N$")
ax1.set_ylabel("Mean Standard Deviation")
ax2.set_ylabel("Mean Standard Deviation")
ax2.set_xlim(ax1.get_xlim())
ax1.legend()
ax2.legend()
fig.savefig(f"stds_all.png")
plt.show()
# -

# ### BOS and padding tokens

# +
n_batch = len(norms_by_pos[0])
n_pos = len(norms_by_pos[0][0])
n_layers = len(norms_by_pos)
plt.figure(figsize=(10,5))
for batch in range(n_batch):
    for pos in range(n_pos):
        #print(f"Batch {batch}, Position {pos}")
        color = plt.cm.viridis(pos/n_pos)
        plt.plot(range(n_layers), [norms_by_pos[i][batch][pos] for i in range(n_layers)], color=color, label=f"Position {pos}", lw=0.1)

# Colorbar
sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=n_pos))
sm.set_array([])
plt.colorbar(sm, ticks=np.arange(0,n_pos+1,1), label="Position")
plt.title(f"Norm of Residual Stream activations after layer $N$ ({model_name})")
plt.xlabel("$N$")
plt.semilogy()
plt.ylabel("L2-Norm")
plt.savefig(f"norms_by_pos_{model_name}.png")
# -

# #### Double check bump coing from paddding tokens
#
# The first few prompts have ~6 tokens so we should see exclusively padding later

# +
n_pos = len(stds[0][0])
n_layers = len(norms_by_pos)
plt.figure(figsize=(10,5))
for batch in range(5):
    for pos in range(n_pos):
        #print(f"Batch {batch}, Position {pos}")
        color = plt.cm.viridis(pos/n_pos)
        plt.plot(range(n_layers), [norms_by_pos[i][batch][pos] for i in range(n_layers)], color=color, label=f"Position {pos}", lw=0.1)

# Colorbar
sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=n_pos))
sm.set_array([])
plt.colorbar(sm, ticks=np.arange(0,n_pos+1,1), label="Position")
plt.title(f"Norm of Residual Stream activations after layer $N$ ({model_name})")
plt.xlabel("$N$")
plt.semilogy()
plt.ylabel("L2-norm")
# -

# ### Theories for the source of the growth

plt.figure(figsize=(5,3))
plt.scatter(np.arange(model.cfg.n_layers)+0.5, meannorms_mid, marker="o", label="After Attn", s=2)
plt.scatter(np.arange(model.cfg.n_layers+1), meannorms, marker="o", label="After MLPs", s=2)
plt.semilogy()
plt.ylim(40, 800)
plt.title("Norm of Residual Stream activations")
plt.xlabel("Layer Index $N$")
plt.ylabel("L2 Norm")
plt.legend()
plt.savefig(f"stds_{model_name}.png")
plt.show()

plt.figure(figsize=(5,3))
plt.scatter(np.arange(model.cfg.n_layers)+0.5, meanstds_mid, marker="o", label="After Attn", s=2)
plt.scatter(np.arange(model.cfg.n_layers+1), meanstds, marker="o", label="After MLPs", s=2)
plt.semilogy()
plt.ylim(1, 20)
plt.title("Mean Standard Deviation of Residual Stream activations")
plt.xlabel("Layer Index $N$")
plt.ylabel("Mean Standard Deviation")
plt.legend()
plt.savefig(f"stds_{model_name}.png")
plt.show()

r = np.arange(model.cfg.n_layers)
fig, ax = plt.subplots(1, 1, figsize=(4,3), constrained_layout=True)
ax.plot(r, meannorms_attn_out, label="attn_out", marker="o", markersize=3)
ax.plot(r+0.5, meannorms_mlp_out, label="mlp_out", marker="o", markersize=3)
#ax.plot(r, np.sqrt(meanstds_pre**2+meanstds_attn_out**2), ls=":", label="S")
#ax.plot(r, np.sqrt(meanstds_mid**2+meanstds_mlp_out**2), ls=":", label="S")
ax.set_title("Residual Stream norm")
ax.set_ylabel("L2 norm")
ax.set_xlabel("Layer index $N$")
ax.plot([5, 41], [40*0.25, 40*0.25*1.045**(41-5)], label=r"Slope $1.045^N$", color="k", ls=":")
ax.plot([5, 41], [40*0.12, 40*0.12*1.045**(41-5)], color="k", ls=":")
ax.set_xlim(0, model.cfg.n_layers-1)
ax.legend()
ax.set_yscale("log")
ax.set_ylim(0.1*40, 2.8*40)
ax.set_xlim(0, model.cfg.n_layers)

# ### Analyzing the model weights to understand the behaviour

# #### Attention

# Compute values

cache2 = model.run_with_cache(prompts)[1]
for key in cache.keys():
    assert torch.allclose(cache[key], cache2[key], atol=1e-5)

# +
print("Model name:", model_name)
assert model.cfg.model_name == model_name

df_OV_scale = pd.DataFrame(columns=["Layer", "Source", "Norm"])

for layer in range(model.cfg.n_layers):
    print(f"Layer {layer} of {model.cfg.n_layers}")
    # b_O
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Bias", model.blocks[layer].attn.b_O.norm(dim=-1).item()]],
                             columns=["Layer", "Source", "Norm"])], ignore_index=True)
    # W_OV
    W_OVs = einsum("head hidden embedout, head embed hidden -> head embed embedout", model.blocks[layer].attn.W_O, model.blocks[layer].attn.W_V)
    # Analytical
    analytic_norm = W_OVs.norm(dim=(-2, -1))
    # Empirical
    random_vector = torch.randn(1000, model.cfg.d_model).to(device)
    random_vector /= random_vector.std(dim=-1, keepdim=True) #LN
    OV_output_empirical = einsum("batch embed, head embed embedout -> batch head embedout", random_vector, W_OVs)
    empirical_norm = OV_output_empirical.norm(dim=-1).mean(dim=0)
    # # Equal?
    assert torch.allclose(empirical_norm, analytic_norm, rtol=0.2), (empirical_norm/analytic_norm)
    # Save result by head
    for head in range(model.cfg.n_heads):
        df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, head, analytic_norm[head].item()]],
                                columns=["Layer", "Source", "Norm"])], ignore_index=True)
    # Empirical sum
    mean_total_norm_increase = OV_output_empirical.sum(dim=1).norm(dim=0).mean(dim=0)
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Sum (random input)", mean_total_norm_increase.item()]],
                             columns=["Layer", "Source", "Norm"])], ignore_index=True)
    # Try using randomly resampled resid mid rather than random vector
    resid_mid = cache[f"blocks.{layer}.hook_resid_mid"][:,:,:]
    attn_weights = cache[f"blocks.{layer}.attn.hook_pattern"][:,:,:,:]
    mean_attn_weights = cache[f"blocks.{layer}.attn.hook_pattern"][:,:,:,:].mean(dim=0)
    OV_output_resid = einsum("batch pos embed, head embed embedout -> batch pos head embedout", resid_mid, W_OVs)
    norm_empirical_using_resid_weighted = (einsum("batch dst head embed, head dst src -> batch src embed", OV_output_resid, mean_attn_weights))[:, 1:6, :].norm(dim=-1).mean(dim=(0, 1))
    norm_empirical_using_resid_uniform = OV_output_resid.sum(dim=-2)[:, 1:6, :].norm(dim=-1).mean(dim=(0, 1))
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Sum (resid_mid, same layer, attn-weighted)", norm_empirical_using_resid_weighted.item()]], columns=["Layer", "Source", "Norm"])], ignore_index=True)
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Sum (resid_mid, same layer, uniform)", norm_empirical_using_resid_uniform.item()]], columns=["Layer", "Source", "Norm"])], ignore_index=True)
    # Full calculation to check output. Note: Padding should be automatically excluded
    norm_using_full_model = ( einsum("batch dst head embed, batch head dst src -> batch src embed", OV_output_resid, attn_weights) + model.blocks[layer].attn.b_O.view(-1, 1, model.cfg.d_model) )[:, 1:, :].norm(dim=-1).mean(dim=(0, 1))
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Sum (full model)", norm_using_full_model.item()]], columns=["Layer", "Source", "Norm"])], ignore_index=True)

fig = px.line(df_OV_scale, x="Layer", y="Norm", color="Source", log_y=True, title="Attention W_OV norms")
fig.show()


# -

# Make plots

# +
meannorms_attn_out = np.load(f"cached_data/meannorms_attn_out_{model_name}.npy")
meannorms_attn_out = np.array(meannorms_attn_out)

plt.figure(figsize=(6,4), constrained_layout=True)

arr = df_OV_scale[df_OV_scale["Source"]=="Bias"]["Norm"].to_numpy()
plt.plot(range(len(arr)), arr, alpha=1, ls="solid", label="L2-norm of b_O", color="pink")

for head in range(25):
    arr = df_OV_scale[df_OV_scale["Source"]==head]["Norm"].to_numpy()
    if head==0:
        plt.plot(range(len(arr)), arr, alpha=0.2, color="grey", label="Frobenius-norm of W_OV (indiv. heads)")
    else:
        plt.plot(range(len(arr)), arr, alpha=0.2, color="grey")

#arr = df_OV_scale[df_OV_scale["Source"]=="Random: Naive sum"]["Norm"].to_numpy()
#plt.plot(range(len(arr)), arr*factor, alpha=1, ls="solid", label="L2-norm of W_OV outputs (summed over heads)", color="tab:orange")

#arr = df_OV_scale[df_OV_scale["Source"]=="Resid: Attn-weighted"]["Std increase"].to_numpy()
#plt.plot(range(len(arr)), arr*factor, alpha=1, ls="--", label="Sum: Attention weighed resid_pre inputs", color="green")

#plt.plot([5, 41], [0.1, 0.1*1.045**(41-5)], label=r"Slope $1.045^N$", color="k", ls=":")
plt.plot(range(len(meannorms_attn_out)), meannorms_attn_out, label="Actual attn_out L2-norm (target)", marker="o", markersize=3, color="tab:blue")

plt.title("Comparing W_OV and b_O norms to attn_out")
plt.xlabel("Layer index $N$")
plt.ylabel("Frobenius and L2 norm")
plt.ylim(0.4, 120)
plt.semilogy()
plt.legend()
# -

# The following calculation seems to be off since Sum (full model) does not match attn_out.

# +
meannorms_attn_out = np.load(f"cached_data/meannorms_attn_out_{model_name}.npy")
meannorms_attn_out = np.array(meannorms_attn_out)

plt.figure(figsize=(6,4), constrained_layout=True)

arr = df_OV_scale[df_OV_scale["Source"]=="Bias"]["Norm"].to_numpy()
plt.plot(range(len(arr)), arr, alpha=1, ls="solid", label="L2-norm of b_O", color="pink")

for head in range(25):
    arr = df_OV_scale[df_OV_scale["Source"]==head]["Norm"].to_numpy()
    if head==0:
        plt.plot(range(len(arr)), arr, alpha=0.2, color="grey", label="Frobenius-norm of W_OV (indiv. heads)")
    else:
        plt.plot(range(len(arr)), arr, alpha=0.2, color="grey")

plt.plot(range(len(meannorms_attn_out)), meannorms_attn_out, label="Actual attn_out L2-norm (target)", marker="o", markersize=3)

for sum in ["Sum (random input)", "Sum (resid_mid, same layer, uniform)", "Sum (resid_mid, same layer, attn-weighted)", "Sum (full model)"]:
    arr = df_OV_scale[df_OV_scale["Source"]==sum]["Norm"].to_numpy()
    plt.plot(range(len(arr)), arr, alpha=1, ls="solid", label=sum)
    

plt.title("Comparing W_OV and b_O norms to attn_out")
plt.xlabel("Layer index $N$")
plt.ylabel("Frobenius and L2 norm")
plt.ylim(0.4, 120)
plt.semilogy()
plt.legend()
# -

# #### MLP

# Is it possible to "sneak through" the number of features? I.e. can we have vectors
# that all have mean 0 std 1 but still differ in the number of features contained?
#
# Say you can have 1 feature x=[-1,1,-1,1]/2 or 2 features y=[-1,1,1,1]/2.
# Say W_in = [[-1,1,-1,1], [-1, 0, 1, 0]], W_out = [randn(4), randn(4)]^T.
#
# Then MLP @ x = [2,0] activates one neuron, and MLP @ y = [1, 1] activates both neurons.
# So you can sneak through the number of neurons information, and could potentially implement
# something where that number exponentially increases. Especially if you add an activation function after this

# Compute values

# +
print("Model name:", model.cfg.model_name)

mlp_out_stds_random = []
mlp_out_stds_embeds = []
mlp_out_stds_embeds_rl = []
gelu_dead_random = []
gelu_dead_embeds = []
gelu_dead_embeds_rl = []
for layer in range(model.cfg.n_layers):

    random_embed = torch.randn(1000, 1, model.cfg.d_model)
    random_embed -= random_embed.mean(dim=-1, keepdim=True)
    random_embed /= random_embed.std(dim=-1, keepdim=True)

    mlp_out = model.blocks[layer].mlp(random_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_random.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", random_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_random.append(gelu_dead_rate)

    authentic_embed = cache[f"blocks.{layer}.hook_resid_mid"][:,1:6,:]
    authentic_embed /= authentic_embed.std(dim=-1, keepdim=True)
    mlp_out = model.blocks[layer].mlp(authentic_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_embeds.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", authentic_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_embeds.append(gelu_dead_rate)

    # Random layer choice
    random_layer = np.random.randint(model.cfg.n_layers)
    authentic_embed = cache[f"blocks.{random_layer}.hook_resid_mid"][:,1:6,:]
    authentic_embed /= authentic_embed.std(dim=-1, keepdim=True)
    mlp_out = model.blocks[layer].mlp(authentic_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_embeds_rl.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", authentic_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_embeds_rl.append(gelu_dead_rate)

# -

# Make plots

# +
blue = '#1f77b4'
orange = '#ff7f0e'
green = '#2ca02c'
c1 = [blue, orange, green]

meanstds_mlp_out = np.load(f"cached_data/meanstds_mlp_out_{model_name}.npy")
std_to_norm = np.sqrt(model.cfg.d_model)

fig, ax = plt.subplots(1, 1, figsize=(6,3), constrained_layout=True)
ax.set_title("Norm of MLP output for random input, compared to norm of mlp_out")
ax.set_xlabel("Layer index $N$")
ax.set_ylabel("L2-norm")
ax.semilogy(np.array(mlp_out_stds_random)*std_to_norm, label="Random input (normalized)", marker="o", markersize=3, color=c1[0])
ax.plot([5, 41], [std_to_norm*0.6, std_to_norm*0.6*1.045**(41-5)], color="k", ls=":")
ax.plot(np.arange(model.cfg.n_layers)+0.5, np.array(meanstds_mlp_out)*std_to_norm, label="mlp_out (target)", marker="o", markersize=3, color=c1[2])
ax.plot([5, 41], [std_to_norm*0.22, std_to_norm*0.22*1.045**(41-5)], label=r"Slope $1.045^N$", color="k", ls=":")
ax.legend()
# -

# # Appendix

# A2: Do the "Theories for the source of the growth" curves add up?

# +
r = np.arange(model.cfg.n_layers)
fig, ax = plt.subplots(1, 1, figsize=(5,4), constrained_layout=True)

meannorms_mid = np.load(f"cached_data/meannorms_mid_{model_name}.npy")
meannorms_post = np.load(f"cached_data/meannorms_post_{model_name}.npy")

 # No line
ax.plot(np.arange(model.cfg.n_layers)+0.5, meannorms_mid, marker="o", label="resid_mid", markersize=2, lw=0)
ax.plot(np.arange(model.cfg.n_layers)+1, meannorms_post, marker="o", label="resid_post", markersize=2, lw=0)

ax.plot(r, meannorms_attn_out, label="attn_out", marker="o", markersize=3, color="tab:blue")
ax.plot(r+0.5, meannorms_mlp_out, label="mlp_out", marker="o", markersize=3, color="tab:orange")
ax.set_title("Norm of resid_mid, resid_pre, attn_out, and mlp_out")
ax.set_ylabel("L2 norm")
ax.set_xlabel("Layer index $N$")
ax.set_xlim(0, model.cfg.n_layers-1)
ax.legend()
ax.set_yscale("log")
ax.set_xlim(0, model.cfg.n_layers)

# -



# +
r = np.arange(model.cfg.n_layers)
fig, ax = plt.subplots(1, 1, figsize=(5,6), constrained_layout=True)

meanstds_pre = np.load(f"cached_data/meanstds_pre_{model_name}.npy")
meanstds_mid = np.load(f"cached_data/meanstds_mid_{model_name}.npy")
meanstds_post = np.load(f"cached_data/meanstds_post_{model_name}.npy")
meanstds_attn_out = np.load(f"cached_data/meanstds_attn_out_{model_name}.npy")
meanstds_mlp_out = np.load(f"cached_data/meanstds_mlp_out_{model_name}.npy")

scaling = 1/1.045**r
ax.plot(r+0.5, meanstds_pre*scaling, label="resid_pre", lw=1)
ax.plot(r+0.5, meanstds_mid*scaling, label="resid_mid", lw=1)
ax.plot(r+1, meanstds_post*scaling, label="resid_post", lw=1)
ax.plot(r, meanstds_attn_out*scaling, label="attn_out", marker="o", markersize=3, color="tab:orange", alpha=0.3)
ax.plot(r+0.5, meanstds_mlp_out*scaling, label="mlp_out", marker="o", markersize=3, color="tab:green", alpha=0.3)

ax.set_ylim(5e-2, 2)
ax.set_xlim(0, model.cfg.n_layers)
ax.set_title("Std of resid_mid, resid_pre, attn_out, and mlp_out")
ax.set_ylabel("Standard deviation $\cdot 1.045^N$")
ax.set_xlabel("Layer index $N$")
ax.set_yscale("log")
ax.legend()


# +
r = np.arange(model.cfg.n_layers)
scaling = 1/1.045**r

meanstds_pre = np.load(f"cached_data/meanstds_pre_{model_name}.npy")
meanstds_mid = np.load(f"cached_data/meanstds_mid_{model_name}.npy")
meanstds_post = np.load(f"cached_data/meanstds_post_{model_name}.npy")
meanstds_attn_out = np.load(f"cached_data/meanstds_attn_out_{model_name}.npy")

fig, ax = plt.subplots(1, 1, figsize=(6,4), constrained_layout=True)
ax.plot(r+0.5, meanstds_pre*scaling, label="resid_pre", lw=1)
ax.plot(r+0.5, meanstds_mid*scaling, label="resid_mid", lw=1)
ax.plot(r+1, meanstds_post*scaling, label="resid_post", lw=1)

resid_mid_uncorrelated_prediction = np.sqrt(meanstds_pre**2+meanstds_attn_out**2)
resid_mid_correlated_prediction = meanstds_pre+meanstds_attn_out
ax.fill_between(r, scaling*resid_mid_uncorrelated_prediction, scaling*resid_mid_correlated_prediction, label="resid_mid range", alpha=0.3, color="tab:orange")


ax.set_ylim(1.2, 1.8)
ax.set_xlim(0, model.cfg.n_layers)
ax.set_title("Std of resid_mid based on correlated addition of resid_pre and attn_out")
ax.set_ylabel("Standard deviation $\cdot 1.045^N$")
ax.set_xlabel("Layer index $N$")
ax.set_yscale("log")
ax.legend(loc="lower right")


# +
r = np.arange(model.cfg.n_layers)
scaling = 1/1.045**r

meanstds_pre = np.load(f"cached_data/meanstds_pre_{model_name}.npy")
meanstds_mid = np.load(f"cached_data/meanstds_mid_{model_name}.npy")
meanstds_post = np.load(f"cached_data/meanstds_post_{model_name}.npy")
meanstds_mlp_out = np.load(f"cached_data/meanstds_mlp_out_{model_name}.npy")

fig, ax = plt.subplots(1, 1, figsize=(6,4), constrained_layout=True)
ax.plot(r+0.5, meanstds_pre*scaling, label="resid_pre", lw=1)
ax.plot(r+0.5, meanstds_mid*scaling, label="resid_mid", lw=1)
ax.plot(r+1, meanstds_post*scaling, label="resid_post", lw=1)

resid_post_uncorrelated_prediction = np.sqrt(meanstds_mid**2+meanstds_mlp_out**2)
resid_post_correlated_prediction = meanstds_mid+meanstds_mlp_out
ax.fill_between(r, scaling*resid_post_uncorrelated_prediction, scaling*resid_post_correlated_prediction, label="resid_post range", alpha=0.3, color="tab:green")


ax.set_ylim(1.2, 1.8)
ax.set_xlim(0, model.cfg.n_layers)
ax.set_title("Std of resid_post based on correlated addition of resid_mid and mlp_out")
ax.set_ylabel("Standard deviation $\cdot 1.045^N$")
ax.set_xlabel("Layer index $N$")
ax.set_yscale("log")
ax.legend(loc="lower right")
# -

# ## A4: MLP

# Weights

# +
df_MLP_scale = pd.DataFrame(columns=["Layer", "Norm", "Source"])
# Matrix based calculation
for layer in range(model.cfg.n_layers):
    Winout = einsum("d_model_in d_mlp, d_mlp d_model_out -> d_model_in d_model_out", model.blocks[layer].mlp.W_in, model.blocks[layer].mlp.W_out)
    Winout_mean_norm_increase = Winout.norm(dim=(-2, -1))
    Win_norm = model.blocks[layer].mlp.W_in.norm(dim=(-2, -1))
    bin_mean_norm_increase = einsum("d_mlp, d_mlp d_model -> d_model", model.blocks[layer].mlp.b_in, model.blocks[layer].mlp.W_out).norm(dim=-1)
    bout_mean_norm_increase = model.blocks[layer].mlp.b_out.norm(dim=-1)
    df_MLP_scale = pd.concat([df_MLP_scale, pd.DataFrame([[layer, Winout_mean_norm_increase.item(), "WinWout"]], columns=["Layer", "Norm", "Source"])], ignore_index=True)
    df_MLP_scale = pd.concat([df_MLP_scale, pd.DataFrame([[layer, bin_mean_norm_increase.item(), "binWout"]], columns=["Layer", "Norm", "Source"])], ignore_index=True)
    df_MLP_scale = pd.concat([df_MLP_scale, pd.DataFrame([[layer, bout_mean_norm_increase.item(), "bout"]], columns=["Layer", "Norm", "Source"])], ignore_index=True)
    df_MLP_scale = pd.concat([df_MLP_scale, pd.DataFrame([[layer, Win_norm.item(), "Win"]], columns=["Layer", "Norm", "Source"])], ignore_index=True)



# +
colors = ["tab:red", "tab:purple", "tab:brown"]
r = np.arange(model.cfg.n_layers)
# Make lower axis half height
fig, [ax, axlow] = plt.subplots(2, 1, figsize=(6,4), constrained_layout=True, sharex=True, gridspec_kw={"height_ratios": [3, 1]})
ax.set_title("Norms of MLP weight matrices and vectors")

WinWout = np.array(df_MLP_scale[df_MLP_scale["Source"]=="WinWout"]["Norm"])
ax.plot(r, WinWout, markersize=3, label=r"$W_{\rm in} \cdot W_{\rm out}$", marker="o", color=colors[0])

Win = np.array(df_MLP_scale[df_MLP_scale["Source"]=="Win"]["Norm"])
ax.plot(r, Win, markersize=3, label=r"$W_{\rm in}$", marker="o", color="tab:cyan")

binWout = np.array(df_MLP_scale[df_MLP_scale["Source"]=="binWout"]["Norm"])
ax.plot(r, binWout, markersize=3, label=r"$b_{\rm in} \cdot W_{\rm out}$", marker="o", color=colors[1])

bout = np.array(df_MLP_scale[df_MLP_scale["Source"]=="bout"]["Norm"])
axlow.plot(r, bout, markersize=3, label=r"$b_{\rm out}$", marker="o", color=colors[2])

ax.plot([5, 41], [80, 80*1.045**(41-5)], label=r"Slope $1.045^N$", color="k", ls=":")
axlow.set_xlabel("Layer index $N$")
ax.legend(loc="upper center", ncol=2)
ax.set_yscale("log")
axlow.set_yscale("log")
axlow.set_ylim(1, 3)
axlow.legend()
ax.set_ylabel("Norm")
axlow.set_ylabel("Norm")

# -

# Activation

# +
print("Model name:", model.cfg.model_name)

mlp_out_stds_random = []
mlp_out_stds_embeds = []
mlp_out_stds_embeds_rl = []
gelu_dead_random = []
gelu_dead_embeds = []
gelu_dead_embeds_rl = []
for layer in range(model.cfg.n_layers):

    random_embed = torch.randn(1000, 1, model.cfg.d_model)
    random_embed -= random_embed.mean(dim=-1, keepdim=True)
    random_embed /= random_embed.std(dim=-1, keepdim=True)

    mlp_out = model.blocks[layer].mlp(random_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_random.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", random_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_random.append(gelu_dead_rate)

    authentic_embed = cache[f"blocks.{layer}.hook_resid_mid"][:,1:6,:]
    authentic_embed /= authentic_embed.std(dim=-1, keepdim=True)
    mlp_out = model.blocks[layer].mlp(authentic_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_embeds.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", authentic_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_embeds.append(gelu_dead_rate)

    # Random layer choice
    random_layer = np.random.randint(model.cfg.n_layers)
    authentic_embed = cache[f"blocks.{random_layer}.hook_resid_mid"][:,1:6,:]
    authentic_embed /= authentic_embed.std(dim=-1, keepdim=True)
    mlp_out = model.blocks[layer].mlp(authentic_embed)
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-3), mlp_out.mean(dim=-1)
    mlp_out_stds_embeds_rl.append(mlp_out.std(dim=-1).mean(dim=(0,1)).item())
    hidden = einsum("batch pos embed, embed hidden -> batch pos hidden", authentic_embed, model.blocks[layer].mlp.W_in) + model.blocks[layer].mlp.b_in
    gelu_dead_rate = (hidden[:,:,:] < 0).float().mean().item()
    gelu_dead_embeds_rl.append(gelu_dead_rate)

# -

print(model_name)
fig, ax = plt.subplots()
ax.set_title(r"Neuron activation rate $R_{\rm GELU}$")
ax.plot(r, 1-np.array(gelu_dead_random), label="Random-input", marker="o", markersize=3)
#ax.plot(r, (1-np.array(gelu_dead_random))/4, label="random unit variance vector/4.", marker="o", markersize=3, alpha=0.5, color=c2[0])
ax.plot(r, 1-np.array(gelu_dead_embeds), label="Resampled resid_mid (same layer)", marker="o", markersize=3)
ax.plot(r, 1-np.array(gelu_dead_embeds_rl), label="Resampled resid_mid (random layer)", marker="o", alpha=0.5, markersize=3)
mean_gelu_alive_fraction_actual = np.load(f"mean_gelu_alive_fraction_{model_name}.npy")
ax.plot(r, mean_gelu_alive_fraction_actual, label=r"Actual $R_{\rm GELU}$from model run (target)", marker="o", markersize=3)
ax.plot([5, 41], [1*1.045**(5-41), 1], color="k", ls=":", label=r"Slope $1.045^N$")
ax.plot([5, 41], [0.07, 0.07*1.045**(41-5)], color="k", ls=":")
ax.set_yscale("log")
ax.legend()
ax.set_ylim(0.01,0.8)










